import cv2
import mediapipe as mp
import numpy as np
from collections import deque
import time
import speech_recognition as sr
import threading

class SmartPlantSystem:
    def __init__(self):
        # åˆå§‹åŒ–åŠ¨ä½œè¯†åˆ«
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        
        # å†å²è®°å½•
        self.nose_y_history = deque(maxlen=15)
        self.nose_x_history = deque(maxlen=15)
        self.wrist_history = deque(maxlen=10)
        self.clap_history = deque(maxlen=10)
        
        # å†·å´æ—¶é—´
        self.last_gesture_time = 0
        self.gesture_cooldown = 2.0
        
        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        self.is_listening = True
        self.latest_speech = ""
        self.conversation_mode = False  # å¯¹è¯æ¨¡å¼æ ‡å¿—
        
        # è°ƒæ•´ç¯å¢ƒå™ªéŸ³
        print("æ­£åœ¨æ ¡å‡†éº¦å…‹é£...")
        try:
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
            print("éº¦å…‹é£æ ¡å‡†å®Œæˆï¼")
        except Exception as e:
            print(f"éº¦å…‹é£åˆå§‹åŒ–å¤±è´¥: {e}")
            print("å°†ç»§ç»­è¿è¡Œï¼Œä½†è¯­éŸ³è¯†åˆ«å¯èƒ½ä¸å¯ç”¨")
    
    def calculate_distance(self, point1, point2):
        """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»"""
        return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)
    
    def detect_wave(self, landmarks, image_width, image_height):
        """æ£€æµ‹æŒ¥æ‰‹æ‰“æ‹›å‘¼"""
        right_wrist = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_wrist = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        
        wrist_pos = None
        is_hand_raised = False
        
        if right_wrist.y < right_shoulder.y and right_wrist.y < nose.y + 0.1:
            wrist_pos = (right_wrist.x * image_width, right_wrist.y * image_height)
            is_hand_raised = True
        elif left_wrist.y < left_shoulder.y and left_wrist.y < nose.y + 0.1:
            wrist_pos = (left_wrist.x * image_width, left_wrist.y * image_height)
            is_hand_raised = True
        
        if is_hand_raised and wrist_pos:
            self.wrist_history.append(wrist_pos)
            
            if len(self.wrist_history) >= 8:
                positions = list(self.wrist_history)
                x_positions = [p[0] for p in positions]
                x_changes = [x_positions[i] - x_positions[i-1] for i in range(1, len(x_positions))]
                
                direction_changes = 0
                for i in range(1, len(x_changes)):
                    if (x_changes[i] > 5 and x_changes[i-1] < -5) or \
                       (x_changes[i] < -5 and x_changes[i-1] > 5):
                        direction_changes += 1
                
                if direction_changes >= 2:
                    return True
        else:
            self.wrist_history.clear()
        
        return False
    
    def detect_nod(self, landmarks, image_height):
        """æ£€æµ‹ç‚¹å¤´"""
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        nose_y = nose.y * image_height
        
        self.nose_y_history.append(nose_y)
        
        if len(self.nose_y_history) >= 12:
            positions = list(self.nose_y_history)
            max_y = max(positions)
            min_y = min(positions)
            movement_range = max_y - min_y
            
            if movement_range < 8:
                return False
            
            peaks = 0
            valleys = 0
            
            for i in range(2, len(positions) - 2):
                if (positions[i] > positions[i-1] + 3 and 
                    positions[i] > positions[i-2] + 2 and
                    positions[i] > positions[i+1] + 3):
                    peaks += 1
                elif (positions[i] < positions[i-1] - 3 and 
                      positions[i] < positions[i-2] - 2 and
                      positions[i] < positions[i+1] - 3):
                    valleys += 1
            
            if peaks >= 1 and valleys >= 1:
                return True
        
        return False
    
    def detect_shake(self, landmarks, image_width):
        """æ£€æµ‹æ‘‡å¤´"""
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        nose_x = nose.x * image_width
        
        self.nose_x_history.append(nose_x)
        
        if len(self.nose_x_history) >= 10:
            positions = list(self.nose_x_history)
            max_x = max(positions)
            min_x = min(positions)
            movement_range = max_x - min_x
            
            if movement_range < 25:
                return False
            
            direction_changes = 0
            for i in range(1, len(positions) - 1):
                if (positions[i] > positions[i-1] + 5 and 
                    positions[i] > positions[i+1] + 5):
                    direction_changes += 1
                elif (positions[i] < positions[i-1] - 5 and 
                      positions[i] < positions[i+1] - 5):
                    direction_changes += 1
            
            if direction_changes >= 2:
                return True
        
        return False
    
    def detect_raise_hands(self, landmarks, image_height):
        """æ£€æµ‹åŒæ‰‹ä¸¾é«˜"""
        left_wrist = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_wrist = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        left_elbow = landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value]
        right_elbow = landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value]
        
        left_hand_raised = (
            left_wrist.y < nose.y and
            left_wrist.y < left_shoulder.y - 0.1 and
            left_elbow.y < left_shoulder.y
        )
        
        right_hand_raised = (
            right_wrist.y < nose.y and
            right_wrist.y < right_shoulder.y - 0.1 and
            right_elbow.y < right_shoulder.y
        )
        
        return left_hand_raised and right_hand_raised
    
    def detect_clap(self, landmarks, image_width, image_height):
        """æ£€æµ‹é¼“æŒåŠ¨ä½œ"""
        left_wrist = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_wrist = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        
        left_wrist_pos = (left_wrist.x * image_width, left_wrist.y * image_height)
        right_wrist_pos = (right_wrist.x * image_width, right_wrist.y * image_height)
        
        hands_distance = self.calculate_distance(left_wrist_pos, right_wrist_pos)
        
        hands_in_position = (
            left_wrist.y > left_shoulder.y - 0.2 and
            left_wrist.y < left_shoulder.y + 0.3 and
            right_wrist.y > right_shoulder.y - 0.2 and
            right_wrist.y < right_shoulder.y + 0.3
        )
        
        if hands_in_position:
            self.clap_history.append(hands_distance)
            
            if len(self.clap_history) >= 8:
                distances = list(self.clap_history)
                min_distance = min(distances)
                max_distance = max(distances)
                
                if max_distance - min_distance > 80:
                    close_count = sum(1 for d in distances if d < min_distance + 40)
                    far_count = sum(1 for d in distances if d > max_distance - 40)
                    
                    if close_count >= 2 and far_count >= 2:
                        return True
        else:
            self.clap_history.clear()
        
        return False
    
    def generate_response(self, text):
        """ç”Ÿæˆå›å¤ï¼ˆç®€å•è§„åˆ™ï¼Œåç»­å¯æ›¿æ¢ä¸ºAI APIï¼‰"""
        text = text.lower()
        
        # ç®€å•çš„è§„åˆ™å“åº”
        if "how are you" in text or "how r u" in text:
            return "I'm doing great! Thanks for asking. How about you?"
        
        elif "what is your name" in text or "your name" in text:
            return "I'm your smart plant assistant. You can call me Planty!"
        
        elif "hello" in text or "hi" in text:
            return "Hello there! How can I assist you?"
        
        elif "help" in text:
            return "I can chat with you! Try asking me questions or just say 'bye bye' when you're done."
        
        elif "thank" in text:
            return "You're welcome! Happy to help!"
        
        elif "weather" in text:
            return "I'm a plant, so I love sunny weather! But I can't check the actual weather for you yet."
        
        elif "water" in text:
            return "Remember to water your plants regularly! But not too much - we don't like soggy roots!"
        
        elif "sing" in text or "song" in text:
            return "I'm a plant, not a singer! But I appreciate good music!"
        
        elif "joke" in text:
            return "Why did the plant go to therapy? Because it had too many deep roots!"
        
        else:
            # é»˜è®¤å›å¤ï¼ˆåç»­å¯æ¥å…¥AI APIï¼‰
            return "I heard you! That's interesting. Tell me more!"
    
    def process_conversation(self, text):
        """å¤„ç†å¯¹è¯å†…å®¹"""
        if not text:
            return None
        
        text_lower = text.lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¼€å¯å¯¹è¯æŒ‡ä»¤
        if not self.conversation_mode:
            if "hello world" in text_lower or "helloworld" in text_lower:
                self.conversation_mode = True
                response = "Hello! I'm your smart plant. How can I help you today?"
                print(f"\nğŸ¤– [å¯¹è¯å¼€å¯] å›å¤: {response}")
                return response
            else:
                # éå¯¹è¯æ¨¡å¼ä¸‹ï¼Œåªè¿”å›None
                return None
        
        # å¯¹è¯æ¨¡å¼ä¸‹å¤„ç†
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é—­å¯¹è¯æŒ‡ä»¤
        if "bye bye" in text_lower or "bye-bye" in text_lower or "goodbye" in text_lower or "good bye" in text_lower:
            response = "Have a good day! Goodbye!"
            print(f"\nğŸ¤– [å¯¹è¯ç»“æŸ] å›å¤: {response}")
            self.conversation_mode = False
            return response
        
        # ç”Ÿæˆå¯¹è¯å“åº”
        response = self.generate_response(text_lower)
        print(f"\nğŸ¤– [å¯¹è¯ä¸­] å›å¤: {response}")
        return response
    
    def process_frame(self, frame):
        """å¤„ç†å•å¸§å›¾åƒ"""
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        results = self.pose.process(image)
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        gesture_detected = None
        current_time = time.time()
        
        if results.pose_landmarks:
            self.mp_drawing.draw_landmarks(
                image,
                results.pose_landmarks,
                self.mp_pose.POSE_CONNECTIONS
            )
            
            landmarks = results.pose_landmarks.landmark
            h, w, _ = image.shape
            
            if current_time - self.last_gesture_time > self.gesture_cooldown:
                if self.detect_raise_hands(landmarks, h):
                    gesture_detected = "Wow"
                    self.last_gesture_time = current_time
                elif self.detect_clap(landmarks, w, h):
                    gesture_detected = "Good"
                    self.last_gesture_time = current_time
                    self.clap_history.clear()
                elif self.detect_wave(landmarks, w, h):
                    gesture_detected = "Hi"
                    self.last_gesture_time = current_time
                    self.wrist_history.clear()
                elif self.detect_nod(landmarks, h):
                    gesture_detected = "Yes"
                    self.last_gesture_time = current_time
                    self.nose_y_history.clear()
                elif self.detect_shake(landmarks, w):
                    gesture_detected = "No"
                    self.last_gesture_time = current_time
                    self.nose_x_history.clear()
        
        return image, gesture_detected
    
    def listen_speech(self):
        """è¯­éŸ³è¯†åˆ«çº¿ç¨‹"""
        while self.is_listening:
            try:
                with self.microphone as source:
                    # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é˜»å¡
                    audio = self.recognizer.listen(source, timeout=2, phrase_time_limit=8)
                
                # è¯†åˆ«è¯­éŸ³
                text = self.recognizer.recognize_google(audio, language='en-US')
                self.latest_speech = text
                print(f"\nğŸ¤ ä½ è¯´: {text}")
                
                # å¤„ç†å¯¹è¯
                response = self.process_conversation(text)
                
                # å¦‚æœæœ‰å›å¤ï¼Œæ›´æ–°æ˜¾ç¤ºï¼ˆåç»­å¯ä»¥æ·»åŠ è¯­éŸ³è¾“å‡ºï¼‰
                if response:
                    self.latest_speech = f"You: {text} | Bot: {response[:30]}..."
                
            except sr.WaitTimeoutError:
                pass  # è¶…æ—¶ï¼Œç»§ç»­ç›‘å¬
            except sr.UnknownValueError:
                pass  # æ— æ³•è¯†åˆ«ï¼Œç»§ç»­ç›‘å¬
            except sr.RequestError as e:
                print(f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}")
                time.sleep(5)
            except Exception:
                pass  # å…¶ä»–é”™è¯¯ï¼Œç»§ç»­ç›‘å¬
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        print("=" * 60)
        print("ğŸŒ± æ™ºèƒ½ç›†æ ½äº¤äº’ç³»ç»Ÿ")
        print("=" * 60)
        print("ğŸ“¹ åŠ¨ä½œè¯†åˆ«:")
        print("  - æŒ¥æ‰‹ â†’ Hi")
        print("  - åŒæ‰‹ä¸¾é«˜ â†’ Wow")
        print("  - é¼“æŒ â†’ Good")
        print("  - ç‚¹å¤´ â†’ Yes")
        print("  - æ‘‡å¤´ â†’ No")
        print("\nğŸ¤ è¯­éŸ³è¯†åˆ«: å·²å¯åŠ¨ (è‹±è¯­)")
        print("ğŸ’¬ å¯¹è¯åŠŸèƒ½:")
        print("  - è¯´ 'hello world' å¼€å¯å¯¹è¯")
        print("  - è¯´ 'bye bye' æˆ– 'goodbye' ç»“æŸå¯¹è¯")
        print("\næŒ‰ 'q' é”®é€€å‡º")
        print("=" * 60)
        
        # å¯åŠ¨è¯­éŸ³è¯†åˆ«çº¿ç¨‹
        speech_thread = threading.Thread(target=self.listen_speech, daemon=True)
        speech_thread.start()
        
        window_name = 'Smart Plant System'
        cv2.namedWindow(window_name)
        
        try:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame = cv2.flip(frame, 1)
                processed_frame, gesture = self.process_frame(frame)
                
                if gesture:
                    print(f"\nğŸ‘‹ åŠ¨ä½œ: {gesture}")
                    cv2.putText(
                        processed_frame,
                        f"Gesture: {gesture}",
                        (10, 50),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.5,
                        (0, 255, 0),
                        3
                    )
                
                # æ˜¾ç¤ºæœ€æ–°çš„è¯­éŸ³è¯†åˆ«ç»“æœ
                if self.latest_speech:
                    cv2.putText(
                        processed_frame,
                        f"Speech: {self.latest_speech[:30]}",
                        (10, processed_frame.shape[0] - 50),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.7,
                        (255, 255, 0),
                        2
                    )
                
                cv2.putText(
                    processed_frame,
                    "Press 'q' to quit",
                    (10, processed_frame.shape[0] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 255),
                    2
                )
                
                cv2.imshow(window_name, processed_frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\nç”¨æˆ·æŒ‰ä¸‹ 'q' é”®ï¼Œæ­£åœ¨é€€å‡º...")
                    break
                
                if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
                    print("\nçª—å£è¢«å…³é—­ï¼Œæ­£åœ¨é€€å‡º...")
                    break
        
        except KeyboardInterrupt:
            print("\n\næ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
        
        finally:
            self.is_listening = False
            cap.release()
            cv2.destroyAllWindows()
            self.pose.close()
            print("ç¨‹åºå·²å®‰å…¨é€€å‡º")

if __name__ == "__main__":
    system = SmartPlantSystem()
    system.run()
