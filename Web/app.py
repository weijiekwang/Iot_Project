"""
æ™ºèƒ½ç›†æ ½Webç›‘æ§ç³»ç»Ÿ
æä¾›æ¹¿åº¦ç›‘æ§å’Œå¯¹è¯æ§åˆ¶ç•Œé¢
æ”¯æŒESP32éŸ³é¢‘å’Œå›¾åƒæ•°æ®å¤„ç†
"""

from flask import Flask, render_template, jsonify, request
from datetime import datetime, timedelta
import json
import os
from threading import Lock
import io
import tempfile
import speech_recognition as sr
import cv2
import numpy as np
import mediapipe as mp
from collections import deque
import time as time_module

app = Flask(__name__)

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_DIR = "data"
HUMIDITY_FILE = os.path.join(DATA_DIR, "humidity_data.json")
CONVERSATION_FILE = os.path.join(DATA_DIR, "conversation_log.json")

# ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)

# çº¿ç¨‹é”
data_lock = Lock()

# å¯¹è¯çŠ¶æ€
conversation_state = {
    "active": False,
    "started_at": None
}

# ============ è¯­éŸ³è¯†åˆ«å¤„ç†ç±» ============
class ServerVoiceRecognizer:
    """æœåŠ¡ç«¯è¯­éŸ³è¯†åˆ«å™¨ - å¤„ç†ESP32ä¸Šä¼ çš„PCMéŸ³é¢‘"""
    def __init__(self):
        self.recognizer = sr.Recognizer()
        self.conversation_mode = False

    def pcm_to_audio(self, pcm_bytes, sample_rate=16000, sample_width=2):
        """å°†PCMå­—èŠ‚è½¬æ¢ä¸ºAudioDataå¯¹è±¡"""
        try:
            audio_data = sr.AudioData(pcm_bytes, sample_rate, sample_width)
            return audio_data
        except Exception as e:
            print(f"PCMè½¬æ¢é”™è¯¯: {e}")
            return None

    def recognize_speech(self, pcm_bytes):
        """è¯†åˆ«PCMéŸ³é¢‘"""
        try:
            audio_data = self.pcm_to_audio(pcm_bytes)
            if not audio_data:
                return None

            # ä½¿ç”¨Googleè¯­éŸ³è¯†åˆ«
            text = self.recognizer.recognize_google(audio_data, language='en-US')
            return text.lower()
        except sr.UnknownValueError:
            print("æ— æ³•è¯†åˆ«è¯­éŸ³")
            return None
        except sr.RequestError as e:
            print(f"è¯†åˆ«æœåŠ¡å‡ºé”™: {e}")
            return None
        except Exception as e:
            print(f"è¯†åˆ«é”™è¯¯: {e}")
            return None

    def process_conversation(self, text):
        """å¤„ç†å¯¹è¯é€»è¾‘"""
        if not text:
            return None, None

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¼€å¯å¯¹è¯æŒ‡ä»¤
        if not conversation_state["active"]:
            if "hello world" in text or "helloworld" in text:
                conversation_state["active"] = True
                conversation_state["started_at"] = datetime.now().isoformat()
                response = "Hello! I'm your smart plant. How can I help you today?"
                return text, response
            else:
                return text, None

        # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é—­å¯¹è¯æŒ‡ä»¤
        if "bye bye" in text or "bye-bye" in text or "goodbye" in text or "good bye" in text:
            response = "Have a good day! Goodbye!"
            return text, response

        # ç”Ÿæˆå›å¤
        response = self.generate_response(text)
        return text, response

    def generate_response(self, text):
        """ç”Ÿæˆå›å¤"""
        text = text.lower()

        if "how are you" in text or "how r u" in text:
            return "I'm doing great! Thanks for asking. How about you?"
        elif "what is your name" in text or "your name" in text:
            return "I'm your smart plant assistant. You can call me Planty!"
        elif "hello" in text or "hi" in text:
            return "Hello there! How can I assist you?"
        elif "help" in text:
            return "I can chat with you! Try asking me questions or just say 'bye bye' when you're done."
        elif "thank" in text:
            return "You're welcome! Happy to help!"
        elif "weather" in text:
            return "I'm a plant, so I love sunny weather! But I can't check the actual weather for you yet."
        elif "water" in text:
            return "Remember to water your plants regularly! But not too much - we don't like soggy roots!"
        elif "sing" in text or "song" in text:
            return "I'm a plant, not a singer! But I appreciate good music!"
        elif "joke" in text:
            return "Why did the plant go to therapy? Because it had too many deep roots!"
        else:
            return "I heard you! That's interesting. Tell me more!"

# ============ åŠ¨ä½œè¯†åˆ«å¤„ç†ç±» ============
class ServerGestureRecognizer:
    """æœåŠ¡ç«¯åŠ¨ä½œè¯†åˆ«å™¨ - å¤„ç†ESP32ä¸Šä¼ çš„JPEGå›¾åƒ"""
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            model_complexity=0  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹
        )

        # ç”¨äºè·Ÿè¸ªå†å²æ•°æ®
        self.nose_y_history = deque(maxlen=15)
        self.nose_x_history = deque(maxlen=15)
        self.wrist_history = deque(maxlen=10)
        self.clap_history = deque(maxlen=10)

        # å†·å´æ—¶é—´
        self.last_gesture_time = 0
        self.gesture_cooldown = 2.0

    def jpeg_to_frame(self, jpeg_bytes):
        """å°†JPEGå­—èŠ‚è½¬æ¢ä¸ºOpenCVå›¾åƒ"""
        try:
            nparr = np.frombuffer(jpeg_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            return frame
        except Exception as e:
            print(f"JPEGè§£ç é”™è¯¯: {e}")
            return None

    def calculate_distance(self, point1, point2):
        """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»"""
        return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)

    def detect_wave(self, landmarks, image_width, image_height):
        """æ£€æµ‹æŒ¥æ‰‹"""
        right_wrist = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_wrist = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]

        wrist_pos = None
        is_hand_raised = False

        if right_wrist.y < right_shoulder.y and right_wrist.y < nose.y + 0.1:
            wrist_pos = (right_wrist.x * image_width, right_wrist.y * image_height)
            is_hand_raised = True
        elif left_wrist.y < left_shoulder.y and left_wrist.y < nose.y + 0.1:
            wrist_pos = (left_wrist.x * image_width, left_wrist.y * image_height)
            is_hand_raised = True

        if is_hand_raised and wrist_pos:
            self.wrist_history.append(wrist_pos)

            if len(self.wrist_history) >= 8:
                positions = list(self.wrist_history)
                x_positions = [p[0] for p in positions]
                x_changes = [x_positions[i] - x_positions[i-1] for i in range(1, len(x_positions))]

                direction_changes = 0
                for i in range(1, len(x_changes)):
                    if (x_changes[i] > 5 and x_changes[i-1] < -5) or \
                       (x_changes[i] < -5 and x_changes[i-1] > 5):
                        direction_changes += 1

                if direction_changes >= 2:
                    return True
        else:
            self.wrist_history.clear()

        return False

    def detect_raise_hands(self, landmarks, image_height):
        """æ£€æµ‹åŒæ‰‹ä¸¾é«˜"""
        left_wrist = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_wrist = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value]
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        left_elbow = landmarks[self.mp_pose.PoseLandmark.LEFT_ELBOW.value]
        right_elbow = landmarks[self.mp_pose.PoseLandmark.RIGHT_ELBOW.value]

        left_hand_raised = (
            left_wrist.y < nose.y and
            left_wrist.y < left_shoulder.y - 0.1 and
            left_elbow.y < left_shoulder.y
        )

        right_hand_raised = (
            right_wrist.y < nose.y and
            right_wrist.y < right_shoulder.y - 0.1 and
            right_elbow.y < right_shoulder.y
        )

        return left_hand_raised and right_hand_raised

    def detect_clap(self, landmarks, image_width, image_height):
        """æ£€æµ‹é¼“æŒ"""
        left_wrist = landmarks[self.mp_pose.PoseLandmark.LEFT_WRIST.value]
        right_wrist = landmarks[self.mp_pose.PoseLandmark.RIGHT_WRIST.value]
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER.value]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER.value]

        left_wrist_pos = (left_wrist.x * image_width, left_wrist.y * image_height)
        right_wrist_pos = (right_wrist.x * image_width, right_wrist.y * image_height)

        hands_distance = self.calculate_distance(left_wrist_pos, right_wrist_pos)

        hands_in_position = (
            left_wrist.y > left_shoulder.y - 0.2 and
            left_wrist.y < left_shoulder.y + 0.3 and
            right_wrist.y > right_shoulder.y - 0.2 and
            right_wrist.y < right_shoulder.y + 0.3
        )

        if hands_in_position:
            self.clap_history.append(hands_distance)

            if len(self.clap_history) >= 8:
                distances = list(self.clap_history)
                min_distance = min(distances)
                max_distance = max(distances)

                if max_distance - min_distance > 80:
                    close_count = sum(1 for d in distances if d < min_distance + 40)
                    far_count = sum(1 for d in distances if d > max_distance - 40)

                    if close_count >= 2 and far_count >= 2:
                        return True
        else:
            self.clap_history.clear()

        return False

    def detect_nod(self, landmarks, image_height):
        """æ£€æµ‹ç‚¹å¤´"""
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        nose_y = nose.y * image_height

        self.nose_y_history.append(nose_y)

        if len(self.nose_y_history) >= 12:
            positions = list(self.nose_y_history)
            max_y = max(positions)
            min_y = min(positions)
            movement_range = max_y - min_y

            if movement_range < 8:
                return False

            peaks = 0
            valleys = 0

            for i in range(2, len(positions) - 2):
                if (positions[i] > positions[i-1] + 3 and
                    positions[i] > positions[i-2] + 2 and
                    positions[i] > positions[i+1] + 3):
                    peaks += 1
                elif (positions[i] < positions[i-1] - 3 and
                      positions[i] < positions[i-2] - 2 and
                      positions[i] < positions[i+1] - 3):
                    valleys += 1

            if peaks >= 1 and valleys >= 1:
                return True

        return False

    def detect_shake(self, landmarks, image_width):
        """æ£€æµ‹æ‘‡å¤´"""
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE.value]
        nose_x = nose.x * image_width

        self.nose_x_history.append(nose_x)

        if len(self.nose_x_history) >= 10:
            positions = list(self.nose_x_history)
            max_x = max(positions)
            min_x = min(positions)
            movement_range = max_x - min_x

            if movement_range < 25:
                return False

            direction_changes = 0
            for i in range(1, len(positions) - 1):
                if (positions[i] > positions[i-1] + 5 and
                    positions[i] > positions[i+1] + 5):
                    direction_changes += 1
                elif (positions[i] < positions[i-1] - 5 and
                      positions[i] < positions[i+1] - 5):
                    direction_changes += 1

            if direction_changes >= 2:
                return True

        return False

    def recognize_gesture(self, jpeg_bytes):
        """è¯†åˆ«æ‰‹åŠ¿"""
        try:
            frame = self.jpeg_to_frame(jpeg_bytes)
            if frame is None:
                return None

            # è½¬æ¢ä¸ºRGB
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False

            # è¿›è¡Œå§¿æ€æ£€æµ‹
            results = self.pose.process(image)

            gesture_detected = None
            current_time = time_module.time()

            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                h, w, _ = frame.shape

                # æ£€æŸ¥å†·å´æ—¶é—´
                if current_time - self.last_gesture_time > self.gesture_cooldown:
                    if self.detect_raise_hands(landmarks, h):
                        gesture_detected = "Wow"
                        self.last_gesture_time = current_time
                    elif self.detect_clap(landmarks, w, h):
                        gesture_detected = "Good"
                        self.last_gesture_time = current_time
                        self.clap_history.clear()
                    elif self.detect_wave(landmarks, w, h):
                        gesture_detected = "Hi"
                        self.last_gesture_time = current_time
                        self.wrist_history.clear()
                    elif self.detect_nod(landmarks, h):
                        gesture_detected = "Yes"
                        self.last_gesture_time = current_time
                        self.nose_y_history.clear()
                    elif self.detect_shake(landmarks, w):
                        gesture_detected = "No"
                        self.last_gesture_time = current_time
                        self.nose_x_history.clear()

            return gesture_detected

        except Exception as e:
            print(f"æ‰‹åŠ¿è¯†åˆ«é”™è¯¯: {e}")
            return None

# åˆ›å»ºå…¨å±€è¯†åˆ«å™¨å®ä¾‹
voice_recognizer = ServerVoiceRecognizer()
gesture_recognizer = ServerGestureRecognizer()

def load_humidity_data():
    """åŠ è½½æ¹¿åº¦æ•°æ®"""
    if os.path.exists(HUMIDITY_FILE):
        try:
            with open(HUMIDITY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_humidity_data(data):
    """ä¿å­˜æ¹¿åº¦æ•°æ®"""
    with data_lock:
        with open(HUMIDITY_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

def load_conversation_log():
    """åŠ è½½å¯¹è¯è®°å½•"""
    if os.path.exists(CONVERSATION_FILE):
        try:
            with open(CONVERSATION_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_conversation_log(log):
    """ä¿å­˜å¯¹è¯è®°å½•"""
    with data_lock:
        with open(CONVERSATION_FILE, 'w', encoding='utf-8') as f:
            json.dump(log, f, ensure_ascii=False, indent=2)

def get_last_24h_humidity():
    """è·å–è¿‡å»24å°æ—¶çš„æ¹¿åº¦æ•°æ®"""
    all_data = load_humidity_data()
    now = datetime.now()
    past_24h = now - timedelta(hours=24)
    
    # ç­›é€‰è¿‡å»24å°æ—¶çš„æ•°æ®
    filtered_data = [
        entry for entry in all_data
        if datetime.fromisoformat(entry['timestamp']) >= past_24h
    ]
    
    return filtered_data

def generate_sample_data():
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    now = datetime.now()
    sample_data = []
    
    # ç”Ÿæˆè¿‡å»24å°æ—¶çš„ç¤ºä¾‹æ•°æ®ï¼ˆæ¯å°æ—¶ä¸€ä¸ªæ•°æ®ç‚¹ï¼‰
    for i in range(24, 0, -1):
        timestamp = now - timedelta(hours=i)
        # æ¨¡æ‹Ÿæ¹¿åº¦æ•°æ®ï¼š60-80ä¹‹é—´æ³¢åŠ¨
        humidity = 65 + (i % 5) * 3 + ((-1) ** i) * 2
        sample_data.append({
            "timestamp": timestamp.isoformat(),
            "humidity": humidity
        })
    
    save_humidity_data(sample_data)
    return sample_data

# åˆå§‹åŒ–ç¤ºä¾‹æ•°æ®
if not os.path.exists(HUMIDITY_FILE):
    generate_sample_data()

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')

@app.route('/api/humidity')
def get_humidity():
    """è·å–æ¹¿åº¦æ•°æ®API"""
    data = get_last_24h_humidity()
    return jsonify({
        "success": True,
        "data": data,
        "count": len(data)
    })

@app.route('/api/conversation/status')
def get_conversation_status():
    """è·å–å¯¹è¯çŠ¶æ€API"""
    return jsonify({
        "success": True,
        "active": conversation_state["active"],
        "started_at": conversation_state["started_at"]
    })

@app.route('/api/conversation/start', methods=['POST'])
def start_conversation():
    """å¼€å¯å¯¹è¯API"""
    conversation_state["active"] = True
    conversation_state["started_at"] = datetime.now().isoformat()
    
    # è®°å½•åˆ°å¯¹è¯æ—¥å¿—
    log = load_conversation_log()
    log.append({
        "timestamp": datetime.now().isoformat(),
        "type": "system",
        "message": "å¯¹è¯æ¨¡å¼å·²å¼€å¯"
    })
    save_conversation_log(log)
    
    return jsonify({
        "success": True,
        "message": "å¯¹è¯å·²å¼€å¯",
        "active": True
    })

@app.route('/api/conversation/stop', methods=['POST'])
def stop_conversation():
    """å…³é—­å¯¹è¯API"""
    conversation_state["active"] = False
    
    # è®°å½•åˆ°å¯¹è¯æ—¥å¿—
    log = load_conversation_log()
    log.append({
        "timestamp": datetime.now().isoformat(),
        "type": "system",
        "message": "å¯¹è¯æ¨¡å¼å·²å…³é—­"
    })
    save_conversation_log(log)
    
    return jsonify({
        "success": True,
        "message": "å¯¹è¯å·²å…³é—­",
        "active": False
    })

@app.route('/api/conversation/log')
def get_conversation_log():
    """è·å–å¯¹è¯è®°å½•API"""
    log = load_conversation_log()
    # åªè¿”å›æœ€è¿‘50æ¡
    recent_log = log[-50:] if len(log) > 50 else log
    return jsonify({
        "success": True,
        "data": recent_log
    })

@app.route('/api/gesture', methods=['POST'])
def log_gesture():
    """æ¥æ”¶ESP32å›¾åƒå¹¶è¯†åˆ«åŠ¨ä½œ"""
    try:
        # æ¥æ”¶JPEGå›¾åƒæ•°æ®
        jpeg_bytes = request.data

        if not jpeg_bytes:
            return jsonify({
                "success": False,
                "error": "No image data received",
                "gesture": None
            }), 400

        print(f"æ¥æ”¶åˆ°å›¾åƒæ•°æ®: {len(jpeg_bytes)} å­—èŠ‚")

        # è¯†åˆ«åŠ¨ä½œ
        gesture = gesture_recognizer.recognize_gesture(jpeg_bytes)

        # å¦‚æœè¯†åˆ«åˆ°åŠ¨ä½œï¼Œè®°å½•åˆ°å¯¹è¯æ—¥å¿—
        if gesture:
            log = load_conversation_log()
            log.append({
                "timestamp": datetime.now().isoformat(),
                "type": "gesture",
                "gesture": gesture,
                "message": f"åŠ¨ä½œ: {gesture}"
            })
            save_conversation_log(log)
            print(f"âœ… è¯†åˆ«åˆ°åŠ¨ä½œ: {gesture}")
        else:
            print("â„¹ï¸  æœªè¯†åˆ«åˆ°åŠ¨ä½œ")

        return jsonify({
            "success": True,
            "gesture": gesture
        })

    except Exception as e:
        print(f"âŒ åŠ¨ä½œè¯†åˆ«é”™è¯¯: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "gesture": None
        }), 500

@app.route('/api/speech', methods=['POST'])
def log_speech():
    """æ¥æ”¶ESP32éŸ³é¢‘å¹¶è¿›è¡Œè¯­éŸ³è¯†åˆ«å’Œå¯¹è¯"""
    try:
        # æ¥æ”¶PCMéŸ³é¢‘æ•°æ®
        pcm_bytes = request.data

        if not pcm_bytes:
            return jsonify({
                "success": False,
                "error": "No audio data received"
            }), 400

        print(f"æ¥æ”¶åˆ°éŸ³é¢‘æ•°æ®: {len(pcm_bytes)} å­—èŠ‚")

        # è¯†åˆ«è¯­éŸ³
        user_text = voice_recognizer.recognize_speech(pcm_bytes)

        if not user_text:
            print("â„¹ï¸  æ— æ³•è¯†åˆ«è¯­éŸ³")
            return jsonify({
                "success": True,
                "user": "",
                "bot": ""
            })

        print(f"âœ… è¯†åˆ«åˆ°: {user_text}")

        # å¤„ç†å¯¹è¯
        user_text, bot_text = voice_recognizer.process_conversation(user_text)

        # è®°å½•åˆ°å¯¹è¯æ—¥å¿—
        log = load_conversation_log()
        if user_text:
            log.append({
                "timestamp": datetime.now().isoformat(),
                "type": "user",
                "message": user_text
            })
        if bot_text:
            log.append({
                "timestamp": datetime.now().isoformat(),
                "type": "bot",
                "message": bot_text
            })
            print(f"ğŸ¤– å›å¤: {bot_text}")
        save_conversation_log(log)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»“æŸå¯¹è¯
        action = ""
        if user_text and ("bye bye" in user_text or "goodbye" in user_text):
            action = "end_conversation"
            conversation_state["active"] = False
            print("âŒ å¯¹è¯å·²ç»“æŸ")

        return jsonify({
            "success": True,
            "user": user_text or "",
            "bot": bot_text or "",
            "action": action
        })

    except Exception as e:
        print(f"âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "user": "",
            "bot": ""
        }), 500

@app.route('/api/humidity/add', methods=['POST'])
def add_humidity():
    """æ·»åŠ æ¹¿åº¦æ•°æ®ï¼ˆä¾›ä¼ æ„Ÿå™¨è°ƒç”¨ï¼‰"""
    data = request.json
    humidity = data.get('humidity', 0)
    
    all_data = load_humidity_data()
    all_data.append({
        "timestamp": datetime.now().isoformat(),
        "humidity": humidity
    })
    save_humidity_data(all_data)
    
    return jsonify({
        "success": True,
        "message": "æ¹¿åº¦æ•°æ®å·²æ·»åŠ "
    })

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸŒ± æ™ºèƒ½ç›†æ ½Webç›‘æ§ç³»ç»Ÿ")
    print("=" * 60)
    print("æœ¬åœ°è®¿é—®åœ°å€: http://localhost:8080")
    print("å±€åŸŸç½‘è®¿é—®åœ°å€: http://[ä½ çš„IP]:8080")
    print("\nğŸ’¡ æç¤º:")
    print("  - å¦‚éœ€å¤–ç½‘è®¿é—®ï¼Œè¯·ä½¿ç”¨ ngrok æˆ– localtunnel")
    print("  - æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("=" * 60)
    app.run(debug=True, host='0.0.0.0', port=8080)
